{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n",
    "#!pip install keras\n",
    "#!pip install torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from vae import VAE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "# Проверка версии TensorFlow\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Case HW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1. \n",
    "Сделать сравнение табличных unsupervised детекторов аномалий (не менее 5) по метрики gini на одном из датасетов на выбор: синтетический и реальный."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Загрузка и анализ датасета. Содержится минимум 3 ячейки с разведочным анализом данных (например, кол-во наблюдений в каждой группе таргета)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "data = pd.read_csv('C:/Users/pavle/OneDrive/Desktop/ml_case/creditcard.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Основная информация о датасете\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\n",
      "0    284315\n",
      "1       492\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Количество наблюдений в каждой группе таргета\n",
    "print(data['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>1.168375e-15</td>\n",
       "      <td>3.416908e-16</td>\n",
       "      <td>-1.379537e-15</td>\n",
       "      <td>2.074095e-15</td>\n",
       "      <td>9.604066e-16</td>\n",
       "      <td>1.487313e-15</td>\n",
       "      <td>-5.556467e-16</td>\n",
       "      <td>1.213481e-16</td>\n",
       "      <td>-2.406331e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.654067e-16</td>\n",
       "      <td>-3.568593e-16</td>\n",
       "      <td>2.578648e-16</td>\n",
       "      <td>4.473266e-15</td>\n",
       "      <td>5.340915e-16</td>\n",
       "      <td>1.683437e-15</td>\n",
       "      <td>-3.660091e-16</td>\n",
       "      <td>-1.227390e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  1.168375e-15  3.416908e-16 -1.379537e-15  2.074095e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   9.604066e-16  1.487313e-15 -5.556467e-16  1.213481e-16 -2.406331e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.654067e-16 -3.568593e-16  2.578648e-16  4.473266e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   5.340915e-16  1.683437e-15 -3.660091e-16 -1.227390e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Распределение признаков\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Инициализация всех алгоритмов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для сравнения мы будем использовать следующие модели:\n",
    "\n",
    "- Isolation Forest\n",
    "- Local Outlier Factor (LOF)\n",
    "- One-Class SVM\n",
    "- Elliptic Envelope\n",
    "- AutoEncoder (используем Keras для создания модели)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">434</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">105</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">450</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m)             │           \u001b[38;5;34m434\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │           \u001b[38;5;34m105\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m)             │           \u001b[38;5;34m112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)             │           \u001b[38;5;34m450\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,101</span> (4.30 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,101\u001b[0m (4.30 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,101</span> (4.30 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,101\u001b[0m (4.30 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "# Инициализация моделей\n",
    "iso_forest = IsolationForest(contamination=0.01)\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.01)\n",
    "one_class_svm = OneClassSVM(nu=0.01)\n",
    "elliptic_envelope = EllipticEnvelope(contamination=0.01)\n",
    "\n",
    "# Создание модели AutoEncoder\n",
    "input_dim = data.shape[1] - 1  # исключаем целевую переменную 'Class'\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoder = Dense(14, activation=\"tanh\")(input_layer)\n",
    "encoder = Dense(7, activation=\"relu\")(encoder)\n",
    "decoder = Dense(14, activation='tanh')(encoder)\n",
    "decoder = Dense(input_dim, activation='relu')(decoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Вывод структуры AutoEncoder\n",
    "autoencoder.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Обучение всех алгоритмов. В ячейке с обучением модели сохранен лог(вывод) процесса обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение данных на тренировочную и тестовую выборки\n",
    "X = data.drop(columns=['Class'])\n",
    "y = data['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение моделей и сохранение лога процесса обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest\n",
    "iso_forest.fit(X_train)\n",
    "iso_forest_preds = iso_forest.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Outlier Factor (LOF)\n",
    "lof_preds = lof.fit_predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Class SVM\n",
    "one_class_svm.fit(X_train)\n",
    "one_class_svm_preds = one_class_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-84.634261877272507 > -84.761857203559572). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-60.957729972955988 > -61.894054710326280). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-144.919832474569802 > -312.810320548962181). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-150.725621789986320 > -151.163725117922525). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-130.671100126326422 > -154.594109304234280). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-119.698619895091483 > -215.043284355890592). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-120.532434189572797 > -218.047259295332282). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-117.818563156352411 > -215.402694992729039). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-250.092412238719419 > -279.798839271590282). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-116.374111507305045 > -215.328271485708598). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-150.204204374295983 > -168.037210094666847). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-153.872174288702411 > -154.074770058992499). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-153.876100646295384 > -157.032562308791995). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-113.744072383242795 > -215.963383136478370). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-117.718880656470304 > -214.368774367194476). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-148.143732364153323 > -169.383663160075315). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-153.413434572370875 > -165.573412929362718). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-143.893870430869327 > -161.000935964661721). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-221.813477946002081 > -223.916359169770516). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-150.498344958609863 > -150.566671532421367). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-116.232177797813137 > -213.835992746664829). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-134.407074433605914 > -136.742583196433429). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-161.433396911207979 > -161.712784512264562). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-160.845887908028033 > -163.247382528186364). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-118.913358752958331 > -223.186216244847941). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-120.961954513760844 > -224.189768131258745). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-161.301192734749293 > -162.137813996531236). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-119.093053909130006 > -218.110227629965948). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-146.398711078328631 > -162.827953393718445). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-207.460465454484563 > -223.045022508499557). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-208.673259579367596 > -222.340495557722164). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-205.636426625917892 > -246.806474656595014). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-143.485860535233485 > -216.718358041114357). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-141.788943643948897 > -219.451701986466162). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-144.110193454302703 > -165.833627560578918). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-140.193888831110030 > -140.682785412756346). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-123.319752017098693 > -153.961644405427080). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-154.604948819604203 > -156.071716427796900). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-114.020580072066679 > -216.486828599798400). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-167.110288630417585 > -170.165884018359009). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-149.907799956934099 > -150.321494008781542). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-118.837072628591415 > -221.789048251934673). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-150.814205640469879 > -152.124007903254551). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-234.352746786859171 > -280.305870347885445). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-122.399215704918134 > -215.170577198041144). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-201.073615986213497 > -222.033274877722249). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-157.125324370545627 > -157.684525755158489). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-162.424524492846388 > -218.785424923409693). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-142.180043760738471 > -163.924229179457939). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-145.577985247552533 > -160.770892941481605). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-142.794964006888165 > -144.921021031488550). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-126.288132382291849 > -131.000679113019572). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-114.176678451055579 > -213.211391446453604). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-134.978936286594404 > -159.883606586410224). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-201.119640053529878 > -215.940448631026129). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-124.298443819034958 > -197.403628440445942). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-141.492649542268055 > -167.693039908045506). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-144.915871264301501 > -161.362342201415260). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-144.919832474569802 > -312.810320548962181). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-136.836833275608853 > -162.429775444502923). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-146.338891349732108 > -168.379113848562497). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-140.616917815299388 > -159.492946640541390). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-142.223410597928904 > -165.428470102411552). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-152.376931155275429 > -156.522725914675732). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-205.747508426600291 > -265.647654176977198). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-168.551437740200669 > -186.718130306343994). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-118.605913796697550 > -215.524371360076373). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-125.231643416534226 > -125.915281861480622). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-166.358719711921054 > -183.452548086568925). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-168.816145226536889 > -186.759550923559260). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-191.671494828487056 > -218.292747759455438). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-116.989977695045297 > -214.624047654264558). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-223.596984055314948 > -226.293291220039777). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-117.678895243076425 > -216.184291255114431). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-147.380424054739251 > -192.373933374079741). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-113.046013516981162 > -217.152629783024423). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-149.686562454726413 > -149.734862972586683). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-134.052055878868458 > -156.509354111597247). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-203.020576787690970 > -227.056167528100644). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-203.679503324612881 > -224.668905036289033). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-167.400931444494347 > -246.176355032398391). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-250.567468997913551 > -253.552652620429541). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-139.992932248268858 > -162.132300795655567). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-135.998606296493534 > -154.098693437195720). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-202.598927207071654 > -219.557621501060737). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-152.682492589377716 > -166.197926689128423). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-202.506660589649982 > -277.157631482710087). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-223.871280152877745 > -225.878657750239029). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-147.957871596056577 > -148.069120873915068). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-144.919832474569802 > -312.810320548962181). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-142.704892169780209 > -169.240679131182929). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-183.102707823311334 > -220.273655897944792). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-223.163674259997379 > -226.063680694396282). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-196.552815466078812 > -254.555546140367056). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-218.133175297091668 > -218.323047746868696). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-154.863949700163715 > -155.478880793835714). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-167.628698972047090 > -210.199615223453662). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-205.926225477014043 > -224.366123907238205). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-127.593558704184360 > -128.185134294317038). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-203.451749143569856 > -224.246565040964924). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-153.199313541265951 > -169.259702985244303). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-112.569972634927723 > -217.205855400654798). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-114.187160223216409 > -219.077603230430299). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-150.248290331359243 > -219.927378984718047). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-200.714273604989756 > -221.975518054449196). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-119.153498436454385 > -216.728827625449100). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-119.268164718317507 > -215.344745531188124). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-135.189590594994399 > -135.401312935606256). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-146.017852512080992 > -146.749416662459936). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-223.278255182379581 > -225.962442354423075). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-150.316571690105633 > -150.419798410651453). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-141.275740578135128 > -161.473115910437002). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-112.233981777202501 > -217.949154214689457). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-147.638039636040503 > -164.039902079028053). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-155.860597336700550 > -158.178867672913952). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-161.943868592868427 > -215.331732306682255). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-205.747508426600291 > -265.647654176977198). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-156.186515980583664 > -168.447134357927268). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-160.064496334871620 > -218.054330420229235). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-192.740159442568711 > -218.171011810308158). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-119.875814295385339 > -219.124838392568421). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-137.941382854325582 > -139.672961372256395). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-221.813477946002081 > -224.645547513009149). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-205.165840639385635 > -217.408199499902878). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-144.697511248043128 > -145.144417333632333). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-156.462655690688280 > -170.547496956638270). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-162.017564694948533 > -216.685360722790250). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-203.609124910179986 > -276.192553862413263). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-202.530861360186890 > -225.994313001505361). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-158.538140162648318 > -159.775092995171093). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-144.919832474569802 > -312.810320548962181). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-147.204505178538000 > -164.467878512892895). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-156.189986631237673 > -166.102486919923081). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-168.451621108388650 > -169.561025320160695). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-211.435749769086129 > -248.826638570497551). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-144.919832474569802 > -312.810320548962181). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-121.416524186713986 > -218.154171000308736). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-144.919832474569802 > -312.810320548962181). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-111.537946235614015 > -211.863825530317769). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-116.436197026563619 > -216.052693296958864). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-149.686562454726413 > -149.961047736182053). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-166.528433471379174 > -176.792907399086175). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-115.542120304028174 > -219.223887040450137). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-205.692765493396251 > -224.626618316146050). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-135.891891324768778 > -158.798165122793904). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-221.285899860950224 > -225.004105664248385). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-141.103788260894675 > -141.322532134380452). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-119.763061682662709 > -218.050570029136907). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-149.356107735178085 > -165.576857053524947). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-146.656256698443741 > -147.644954305328753). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-219.901337993317895 > -223.496017953002564). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-144.919832474569802 > -312.810320548962181). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-157.624841391637773 > -158.830977484944242). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-154.458326807252377 > -154.858752291649523). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-156.622670829227729 > -167.180434552331093). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-148.530206153375559 > -148.744781449988977). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-132.630196634106738 > -133.264140633185690). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-161.019920515988389 > -214.950915034994978). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-120.081911483394236 > -215.830461108791809). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-202.437568233763443 > -249.937495033280499). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-221.813477946002081 > -225.506007902524971). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-250.092412238719419 > -281.499701856241870). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-122.554753916560529 > -215.832341663408727). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-147.747780697812829 > -150.567102158930226). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-142.167225753476373 > -142.813638768555364). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-130.617969223038017 > -130.973569497372637). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-221.515929158113067 > -225.615836090968344). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-157.998791453479924 > -158.299015766148756). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-112.855912826136503 > -214.554571817924653). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-232.225778585042633 > -278.113711012825547). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-222.038479431360457 > -253.468219177783567). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-131.215330519294241 > -131.231433488149037). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-130.000377869498891 > -130.153583894666639). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-167.110288630417585 > -168.941501592764979). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-132.609317373867782 > -208.118988602723391). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-120.026884662390714 > -217.503878434648072). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-144.753313698852565 > -160.945047746101352). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-146.575874138911132 > -162.273681455021062). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-143.530875698500637 > -144.631773005447826). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-91.830414386863538 > -207.883663963963272). You may want to try with a higher value of support_fraction (current value: 0.500).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:186: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-135.298110341412070 > -136.300616874068766). You may want to try with a higher value of support_fraction (current value: 0.500).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Elliptic Envelope\n",
    "elliptic_envelope.fit(X_train)\n",
    "elliptic_envelope_preds = elliptic_envelope.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 611us/step - loss: 375021824.0000 - val_loss: 373544544.0000\n",
      "Epoch 2/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 496us/step - loss: 374257632.0000 - val_loss: 373475488.0000\n",
      "Epoch 3/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507us/step - loss: 374761568.0000 - val_loss: 373407488.0000\n",
      "Epoch 4/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step - loss: 374064736.0000 - val_loss: 373339776.0000\n",
      "Epoch 5/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505us/step - loss: 373539872.0000 - val_loss: 373272288.0000\n",
      "Epoch 6/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 509us/step - loss: 374715808.0000 - val_loss: 373204928.0000\n",
      "Epoch 7/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498us/step - loss: 374171584.0000 - val_loss: 373137376.0000\n",
      "Epoch 8/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505us/step - loss: 373615712.0000 - val_loss: 373069984.0000\n",
      "Epoch 9/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - loss: 373498368.0000 - val_loss: 373002784.0000\n",
      "Epoch 10/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - loss: 374173984.0000 - val_loss: 372935296.0000\n",
      "Epoch 11/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step - loss: 374520064.0000 - val_loss: 372868032.0000\n",
      "Epoch 12/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 508us/step - loss: 374255488.0000 - val_loss: 372800576.0000\n",
      "Epoch 13/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 509us/step - loss: 373461248.0000 - val_loss: 372733216.0000\n",
      "Epoch 14/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step - loss: 374838112.0000 - val_loss: 372665984.0000\n",
      "Epoch 15/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - loss: 374871200.0000 - val_loss: 372598592.0000\n",
      "Epoch 16/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 506us/step - loss: 374394656.0000 - val_loss: 372531456.0000\n",
      "Epoch 17/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500us/step - loss: 375079552.0000 - val_loss: 372464192.0000\n",
      "Epoch 18/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505us/step - loss: 373156576.0000 - val_loss: 372396896.0000\n",
      "Epoch 19/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505us/step - loss: 373671424.0000 - val_loss: 372329536.0000\n",
      "Epoch 20/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505us/step - loss: 372731200.0000 - val_loss: 372262144.0000\n",
      "Epoch 21/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505us/step - loss: 372498624.0000 - val_loss: 372194976.0000\n",
      "Epoch 22/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505us/step - loss: 374376960.0000 - val_loss: 372127680.0000\n",
      "Epoch 23/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - loss: 373815296.0000 - val_loss: 372060480.0000\n",
      "Epoch 24/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 502us/step - loss: 372199360.0000 - val_loss: 371993120.0000\n",
      "Epoch 25/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - loss: 373452128.0000 - val_loss: 371926144.0000\n",
      "Epoch 26/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 497us/step - loss: 374204512.0000 - val_loss: 371858816.0000\n",
      "Epoch 27/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500us/step - loss: 373291776.0000 - val_loss: 371791456.0000\n",
      "Epoch 28/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 497us/step - loss: 373025856.0000 - val_loss: 371724416.0000\n",
      "Epoch 29/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 497us/step - loss: 372649984.0000 - val_loss: 371657152.0000\n",
      "Epoch 30/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500us/step - loss: 372795840.0000 - val_loss: 371590016.0000\n",
      "Epoch 31/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498us/step - loss: 373722080.0000 - val_loss: 371522784.0000\n",
      "Epoch 32/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 495us/step - loss: 372717696.0000 - val_loss: 371455680.0000\n",
      "Epoch 33/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499us/step - loss: 373046400.0000 - val_loss: 371388512.0000\n",
      "Epoch 34/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 496us/step - loss: 371520064.0000 - val_loss: 371321376.0000\n",
      "Epoch 35/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 509us/step - loss: 372229504.0000 - val_loss: 371253984.0000\n",
      "Epoch 36/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498us/step - loss: 372760544.0000 - val_loss: 371186944.0000\n",
      "Epoch 37/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 493us/step - loss: 372018016.0000 - val_loss: 371119712.0000\n",
      "Epoch 38/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500us/step - loss: 372594912.0000 - val_loss: 371052576.0000\n",
      "Epoch 39/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 496us/step - loss: 372076800.0000 - val_loss: 370985632.0000\n",
      "Epoch 40/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500us/step - loss: 372883232.0000 - val_loss: 370918432.0000\n",
      "Epoch 41/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500us/step - loss: 371581632.0000 - val_loss: 370851232.0000\n",
      "Epoch 42/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 497us/step - loss: 371938240.0000 - val_loss: 370784160.0000\n",
      "Epoch 43/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 503us/step - loss: 373195648.0000 - val_loss: 370717120.0000\n",
      "Epoch 44/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500us/step - loss: 371166048.0000 - val_loss: 370650016.0000\n",
      "Epoch 45/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 496us/step - loss: 372545920.0000 - val_loss: 370582976.0000\n",
      "Epoch 46/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499us/step - loss: 371732064.0000 - val_loss: 370515968.0000\n",
      "Epoch 47/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 494us/step - loss: 370924992.0000 - val_loss: 370448800.0000\n",
      "Epoch 48/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 497us/step - loss: 372086240.0000 - val_loss: 370381792.0000\n",
      "Epoch 49/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 496us/step - loss: 372987424.0000 - val_loss: 370314656.0000\n",
      "Epoch 50/50\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step - loss: 371329984.0000 - val_loss: 370247584.0000\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 318us/step\n"
     ]
    }
   ],
   "source": [
    "# Обучение AutoEncoder\n",
    "autoencoder.fit(X_train, X_train, epochs=50, batch_size=256, validation_split=0.2, verbose=1)\n",
    "autoencoder_preds = autoencoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Вывод метрики для всех алгоритмов на тестовой выборке в единую табличку. Размер тестовой выборки должен составлять не менее 20% от исходной выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Model      Gini\n",
      "0      Isolation Forest  0.910465\n",
      "1  Local Outlier Factor  0.199898\n",
      "2         One-Class SVM  0.056804\n",
      "3     Elliptic Envelope  0.867955\n",
      "4           AutoEncoder -0.100779\n"
     ]
    }
   ],
   "source": [
    "# Функция для вычисления Gini\n",
    "def gini(y_true, y_prob):\n",
    "    return 2 * roc_auc_score(y_true, y_prob) - 1\n",
    "\n",
    "# Преобразование предсказаний для всех моделей\n",
    "iso_forest_scores = -iso_forest.decision_function(X_test)\n",
    "lof_scores = -lof.negative_outlier_factor_\n",
    "one_class_svm_scores = -one_class_svm.decision_function(X_test)\n",
    "elliptic_envelope_scores = -elliptic_envelope.decision_function(X_test)\n",
    "autoencoder_scores = np.mean(np.power(X_test - autoencoder_preds, 2), axis=1)\n",
    "\n",
    "# Вычисление метрики Gini для всех моделей\n",
    "iso_forest_gini = gini(y_test, iso_forest_scores)\n",
    "lof_gini = gini(y_test, lof_scores)\n",
    "one_class_svm_gini = gini(y_test, one_class_svm_scores)\n",
    "elliptic_envelope_gini = gini(y_test, elliptic_envelope_scores)\n",
    "autoencoder_gini = gini(y_test, autoencoder_scores)\n",
    "\n",
    "# Создание таблицы с результатами\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Isolation Forest', 'Local Outlier Factor', 'One-Class SVM', 'Elliptic Envelope', 'AutoEncoder'],\n",
    "    'Gini': [iso_forest_gini, lof_gini, one_class_svm_gini, elliptic_envelope_gini, autoencoder_gini]\n",
    "})\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты показывают эффективность различных моделей обнаружения аномалий по метрике Gini. \n",
    "\n",
    "Isolation Forest (Gini = 0.910465): показывает самый высокий результат по метрике Gini. Модель эффективно различает нормальные и аномальные транзакции.\n",
    "\n",
    "Elliptic Envelope (Gini = 0.867955): также показала высокий результат, что указывает на ее способность хорошо справляться с задачей обнаружения аномалий на данном датасете. Этот алгоритм предполагает, что данные имеют гауссово распределение, что, возможно, близко к истине для этого набора данных.\n",
    "\n",
    "Local Outlier Factor (LOF) (Gini = 0.199898): показал значительно более низкий результат по сравнению с Isolation Forest и Elliptic Envelope. Это может свидетельствовать о том, что LOF менее эффективен для этого конкретного набора данных или требует дополнительной настройки параметров. Может требоваться больше соседей для более точной оценки плотности. Данные могут не соответствовать предположениям модели.\n",
    "\n",
    "One-Class SVM (Gini = 0.056804): показывает почти нулевой результат, что означает, что модель почти не различает нормальные и аномальные транзакции. Это может быть связано с недостаточной настройкой гиперпараметров или неадекватной моделью для данного типа данных. Модель может не подходить для высокоразмерных данных. Необходима настройка параметров, таких как ядро (kernel) и параметр ν (nu).\n",
    "\n",
    "AutoEncoder (Gini = -0.100779): показал отрицательный результат, что является очень плохим показателем. Это может говорить о том, что модель AutoEncoder не смогла обучиться эффективно из-за недостаточного количества данных, неправильной архитектуры или гиперпараметров, либо из-за необходимости более тщательной настройки. Неправильная архитектура нейронной сети (например, недостаточная глубина или количество нейронов). Плохое качество данных для обучения (например, несбалансированные данные).\n",
    "\n",
    "Вывод: Isolation Forest и Elliptic Envelope показали хорошие результаты и могут быть использованы в дальнейшем для этого набора данных.\n",
    "Local Outlier Factor, One-Class SVM, и AutoEncoder требуют дополнительной настройки и анализа. Возможно, стоит рассмотреть альтернативные архитектуры для AutoEncoder или изменить гиперпараметры для SVM и LOF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2. \n",
    "В библиотеке PYOD VAE написан с использованием keras. Реализуйте VAE на torch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализация VAE: https://github.com/yzhao062/pyod/blob/master/pyod/models/vae.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Загрузка и анализ датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "data = pd.read_csv('C:/Users/pavle/OneDrive/Desktop/ml_case/creditcard.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# Разведочный анализ данных\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Time            V1            V2            V3            V4  \\\n",
      "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
      "mean    94813.859575  1.168375e-15  3.416908e-16 -1.379537e-15  2.074095e-15   \n",
      "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
      "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
      "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
      "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
      "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
      "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
      "\n",
      "                 V5            V6            V7            V8            V9  \\\n",
      "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
      "mean   9.604066e-16  1.487313e-15 -5.556467e-16  1.213481e-16 -2.406331e-15   \n",
      "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
      "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
      "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
      "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
      "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
      "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
      "\n",
      "       ...           V21           V22           V23           V24  \\\n",
      "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
      "mean   ...  1.654067e-16 -3.568593e-16  2.578648e-16  4.473266e-15   \n",
      "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
      "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
      "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
      "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
      "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
      "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
      "\n",
      "                V25           V26           V27           V28         Amount  \\\n",
      "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
      "mean   5.340915e-16  1.683437e-15 -3.660091e-16 -1.227390e-16      88.349619   \n",
      "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
      "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
      "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
      "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
      "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
      "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
      "\n",
      "               Class  \n",
      "count  284807.000000  \n",
      "mean        0.001727  \n",
      "std         0.041527  \n",
      "min         0.000000  \n",
      "25%         0.000000  \n",
      "50%         0.000000  \n",
      "75%         0.000000  \n",
      "max         1.000000  \n",
      "\n",
      "[8 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\n",
      "0    284315\n",
      "1       492\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Визуализация количества наблюдений в каждой группе таргета\n",
    "data['Class'].value_counts().plot(kind='bar')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение данных на тренировочную и тестовую выборки\n",
    "X = data.drop(columns=['Class'])\n",
    "y = data['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартизация данных\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Реализовать VAE на torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.3.0-cp311-cp311-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\pavle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\pavle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.9.0)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.12.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pavle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.2)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2024.6.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting mkl<=2021.4.0,>=2021.1.1 (from torch)\n",
      "  Downloading mkl-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Collecting intel-openmp==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch)\n",
      "  Downloading intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting tbb==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch)\n",
      "  Downloading tbb-2021.12.0-py3-none-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pavle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Collecting mpmath<1.4.0,>=1.1.0 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.3.0-cp311-cp311-win_amd64.whl (159.8 MB)\n",
      "   ---------------------------------------- 0.0/159.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/159.8 MB 1.6 MB/s eta 0:01:40\n",
      "   ---------------------------------------- 0.2/159.8 MB 2.5 MB/s eta 0:01:05\n",
      "   ---------------------------------------- 0.5/159.8 MB 3.9 MB/s eta 0:00:41\n",
      "   ---------------------------------------- 1.0/159.8 MB 5.9 MB/s eta 0:00:27\n",
      "   ---------------------------------------- 1.6/159.8 MB 7.4 MB/s eta 0:00:22\n",
      "    --------------------------------------- 2.4/159.8 MB 8.9 MB/s eta 0:00:18\n",
      "    --------------------------------------- 3.5/159.8 MB 11.2 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 4.7/159.8 MB 13.0 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 6.1/159.8 MB 14.9 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 7.3/159.8 MB 16.0 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 8.7/159.8 MB 17.4 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 10.0/159.8 MB 18.3 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 10.8/159.8 MB 22.6 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 11.5/159.8 MB 24.3 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 12.4/159.8 MB 23.4 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 13.6/159.8 MB 25.2 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 14.4/159.8 MB 24.2 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 15.8/159.8 MB 24.2 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 16.7/159.8 MB 23.4 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 17.6/159.8 MB 22.6 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 18.4/159.8 MB 22.6 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 19.8/159.8 MB 21.9 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 20.7/159.8 MB 21.9 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 21.7/159.8 MB 21.8 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 22.9/159.8 MB 22.6 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 24.0/159.8 MB 22.6 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 25.3/159.8 MB 22.6 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 26.6/159.8 MB 23.4 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 27.7/159.8 MB 24.2 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 28.9/159.8 MB 24.3 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 30.1/159.8 MB 25.1 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 31.2/159.8 MB 26.2 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 31.9/159.8 MB 25.2 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 33.1/159.8 MB 25.2 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 34.2/159.8 MB 25.2 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 35.5/159.8 MB 25.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 36.8/159.8 MB 25.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 38.0/159.8 MB 25.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 39.3/159.8 MB 26.2 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 40.4/159.8 MB 25.1 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 41.6/159.8 MB 25.2 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 42.8/159.8 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 44.1/159.8 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 45.3/159.8 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 46.4/159.8 MB 27.3 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 47.7/159.8 MB 27.3 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 49.0/159.8 MB 27.3 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 50.2/159.8 MB 27.3 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 51.4/159.8 MB 27.3 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 52.1/159.8 MB 26.2 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 52.8/159.8 MB 24.2 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 54.1/159.8 MB 24.3 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 55.4/159.8 MB 24.2 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 56.6/159.8 MB 24.2 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 57.8/159.8 MB 24.2 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 59.1/159.8 MB 24.2 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 60.2/159.8 MB 24.2 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 61.4/159.8 MB 24.2 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 62.3/159.8 MB 23.4 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 63.4/159.8 MB 25.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 64.6/159.8 MB 26.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 65.8/159.8 MB 25.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 67.0/159.8 MB 25.2 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 68.1/159.8 MB 25.2 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 69.3/159.8 MB 25.2 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 70.5/159.8 MB 25.2 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 71.6/159.8 MB 25.2 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 72.9/159.8 MB 26.2 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 74.1/159.8 MB 26.2 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 74.9/159.8 MB 25.1 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 76.0/159.8 MB 25.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 77.2/159.8 MB 24.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 78.4/159.8 MB 25.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 79.6/159.8 MB 24.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 80.7/159.8 MB 25.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 82.0/159.8 MB 24.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 83.3/159.8 MB 24.2 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 84.0/159.8 MB 26.2 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 85.3/159.8 MB 27.3 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 86.9/159.8 MB 27.3 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 88.1/159.8 MB 27.3 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 88.9/159.8 MB 27.3 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 90.5/159.8 MB 26.2 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 91.7/159.8 MB 27.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 92.8/159.8 MB 26.2 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 93.9/159.8 MB 25.2 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 95.1/159.8 MB 25.2 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 96.3/159.8 MB 25.1 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 97.1/159.8 MB 25.2 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 98.2/159.8 MB 26.2 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 99.8/159.8 MB 26.2 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 100.6/159.8 MB 26.2 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 101.7/159.8 MB 26.2 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 103.4/159.8 MB 26.2 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 104.6/159.8 MB 26.2 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 105.5/159.8 MB 27.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 107.1/159.8 MB 27.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 107.8/159.8 MB 26.2 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 108.8/159.8 MB 27.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 110.6/159.8 MB 26.2 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 111.7/159.8 MB 27.3 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 112.5/159.8 MB 26.2 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 114.2/159.8 MB 26.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 115.4/159.8 MB 26.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 116.6/159.8 MB 26.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 117.8/159.8 MB 27.3 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 119.0/159.8 MB 31.2 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 120.3/159.8 MB 27.3 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 121.5/159.8 MB 26.2 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 122.7/159.8 MB 27.3 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 123.9/159.8 MB 26.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 125.1/159.8 MB 26.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 126.3/159.8 MB 26.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 127.3/159.8 MB 25.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 128.5/159.8 MB 25.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 129.6/159.8 MB 25.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 130.8/159.8 MB 25.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 131.9/159.8 MB 24.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 133.1/159.8 MB 26.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 134.3/159.8 MB 25.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 135.6/159.8 MB 25.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 136.9/159.8 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 138.1/159.8 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 139.3/159.8 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 140.5/159.8 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 141.7/159.8 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 142.9/159.8 MB 26.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 144.2/159.8 MB 26.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 145.4/159.8 MB 26.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 146.6/159.8 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 147.8/159.8 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 149.0/159.8 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 150.2/159.8 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 151.3/159.8 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 152.6/159.8 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 153.4/159.8 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 155.0/159.8 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  156.2/159.8 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  157.4/159.8 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  158.6/159.8 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 159.8/159.8 MB 19.8 MB/s eta 0:00:00\n",
      "Downloading mkl-2021.4.0-py2.py3-none-win_amd64.whl (228.5 MB)\n",
      "   ---------------------------------------- 0.0/228.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.1/228.5 MB 34.0 MB/s eta 0:00:07\n",
      "   ---------------------------------------- 2.2/228.5 MB 28.6 MB/s eta 0:00:08\n",
      "    --------------------------------------- 3.4/228.5 MB 27.3 MB/s eta 0:00:09\n",
      "    --------------------------------------- 4.6/228.5 MB 26.9 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 5.9/228.5 MB 26.8 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 7.1/228.5 MB 26.8 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 8.3/228.5 MB 27.9 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 9.5/228.5 MB 27.5 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 11.1/228.5 MB 27.3 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 12.3/228.5 MB 27.3 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 13.5/228.5 MB 26.2 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 14.6/228.5 MB 26.2 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 15.7/228.5 MB 26.2 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 16.9/228.5 MB 25.1 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 18.1/228.5 MB 26.2 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 19.3/228.5 MB 26.2 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 20.4/228.5 MB 25.2 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 21.6/228.5 MB 25.2 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 22.9/228.5 MB 25.2 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 24.0/228.5 MB 25.2 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 25.2/228.5 MB 26.2 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 26.4/228.5 MB 26.2 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 27.5/228.5 MB 26.2 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 28.7/228.5 MB 26.2 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 29.9/228.5 MB 26.2 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 31.0/228.5 MB 26.2 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 32.3/228.5 MB 26.2 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 33.4/228.5 MB 26.2 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 34.6/228.5 MB 25.2 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 35.9/228.5 MB 25.2 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 37.1/228.5 MB 26.2 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 38.3/228.5 MB 26.2 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 39.1/228.5 MB 26.2 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 40.8/228.5 MB 26.2 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 41.9/228.5 MB 26.2 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 43.0/228.5 MB 25.2 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 44.2/228.5 MB 26.2 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 45.4/228.5 MB 27.3 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 46.7/228.5 MB 26.2 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 47.9/228.5 MB 26.2 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 49.2/228.5 MB 27.3 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 50.1/228.5 MB 26.2 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 50.4/228.5 MB 24.2 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 50.4/228.5 MB 22.6 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 50.4/228.5 MB 22.6 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 50.4/228.5 MB 22.6 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 50.4/228.5 MB 22.6 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 50.4/228.5 MB 22.6 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 50.4/228.5 MB 22.6 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 50.4/228.5 MB 22.6 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 50.4/228.5 MB 22.6 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 50.4/228.5 MB 22.6 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 51.0/228.5 MB 12.1 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 54.3/228.5 MB 13.1 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 54.7/228.5 MB 12.6 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 55.1/228.5 MB 12.1 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 55.5/228.5 MB 11.7 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 55.8/228.5 MB 11.3 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 56.1/228.5 MB 11.1 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 56.5/228.5 MB 10.7 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 57.0/228.5 MB 10.2 MB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 57.5/228.5 MB 9.9 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 57.9/228.5 MB 9.6 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 58.2/228.5 MB 9.5 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 58.5/228.5 MB 9.1 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 58.9/228.5 MB 8.8 MB/s eta 0:00:20\n",
      "   ---------- ----------------------------- 59.2/228.5 MB 8.6 MB/s eta 0:00:20\n",
      "   ---------- ----------------------------- 59.6/228.5 MB 8.4 MB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 59.9/228.5 MB 8.2 MB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 60.2/228.5 MB 8.0 MB/s eta 0:00:22\n",
      "   ---------- ----------------------------- 60.6/228.5 MB 7.9 MB/s eta 0:00:22\n",
      "   ---------- ----------------------------- 60.8/228.5 MB 11.7 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 61.3/228.5 MB 11.3 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 61.7/228.5 MB 10.9 MB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 62.1/228.5 MB 10.2 MB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 62.5/228.5 MB 9.8 MB/s eta 0:00:17\n",
      "   ----------- ---------------------------- 63.0/228.5 MB 9.4 MB/s eta 0:00:18\n",
      "   ----------- ---------------------------- 63.3/228.5 MB 9.1 MB/s eta 0:00:19\n",
      "   ----------- ---------------------------- 63.8/228.5 MB 8.7 MB/s eta 0:00:19\n",
      "   ----------- ---------------------------- 64.1/228.5 MB 8.4 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 64.5/228.5 MB 8.3 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 64.9/228.5 MB 8.2 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 65.3/228.5 MB 8.3 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 65.7/228.5 MB 8.3 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 66.0/228.5 MB 8.4 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 66.4/228.5 MB 8.3 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 66.7/228.5 MB 8.2 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 67.1/228.5 MB 8.2 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 67.4/228.5 MB 8.1 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 67.7/228.5 MB 8.1 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 68.2/228.5 MB 8.0 MB/s eta 0:00:21\n",
      "   ----------- ---------------------------- 68.5/228.5 MB 8.0 MB/s eta 0:00:21\n",
      "   ------------ --------------------------- 68.9/228.5 MB 8.1 MB/s eta 0:00:20\n",
      "   ------------ --------------------------- 69.2/228.5 MB 8.1 MB/s eta 0:00:20\n",
      "   ------------ --------------------------- 69.4/228.5 MB 8.0 MB/s eta 0:00:20\n",
      "   ------------ --------------------------- 69.7/228.5 MB 8.1 MB/s eta 0:00:20\n",
      "   ------------ --------------------------- 70.3/228.5 MB 8.2 MB/s eta 0:00:20\n",
      "   ------------ --------------------------- 70.8/228.5 MB 8.3 MB/s eta 0:00:20\n",
      "   ------------ --------------------------- 71.2/228.5 MB 8.3 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 71.5/228.5 MB 8.3 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 71.8/228.5 MB 8.2 MB/s eta 0:00:20\n",
      "   ------------ --------------------------- 72.1/228.5 MB 8.1 MB/s eta 0:00:20\n",
      "   ------------ --------------------------- 72.4/228.5 MB 8.1 MB/s eta 0:00:20\n",
      "   ------------ --------------------------- 72.8/228.5 MB 8.0 MB/s eta 0:00:20\n",
      "   ------------ --------------------------- 73.2/228.5 MB 7.9 MB/s eta 0:00:20\n",
      "   ------------ --------------------------- 73.5/228.5 MB 7.9 MB/s eta 0:00:20\n",
      "   ------------ --------------------------- 73.8/228.5 MB 7.9 MB/s eta 0:00:20\n",
      "   ------------ --------------------------- 74.2/228.5 MB 7.9 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 74.6/228.5 MB 7.9 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 74.9/228.5 MB 7.7 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 75.2/228.5 MB 7.9 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 75.6/228.5 MB 7.7 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 75.9/228.5 MB 7.7 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 76.2/228.5 MB 7.6 MB/s eta 0:00:21\n",
      "   ------------- -------------------------- 76.6/228.5 MB 7.6 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 76.8/228.5 MB 7.6 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 77.1/228.5 MB 7.5 MB/s eta 0:00:21\n",
      "   ------------- -------------------------- 77.4/228.5 MB 7.5 MB/s eta 0:00:21\n",
      "   ------------- -------------------------- 77.8/228.5 MB 7.5 MB/s eta 0:00:21\n",
      "   ------------- -------------------------- 78.2/228.5 MB 7.5 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 78.6/228.5 MB 7.5 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 78.6/228.5 MB 7.6 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 78.8/228.5 MB 7.2 MB/s eta 0:00:21\n",
      "   ------------- -------------------------- 79.0/228.5 MB 7.2 MB/s eta 0:00:21\n",
      "   ------------- -------------------------- 79.5/228.5 MB 7.3 MB/s eta 0:00:21\n",
      "   -------------- ------------------------- 80.0/228.5 MB 7.4 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 80.5/228.5 MB 7.4 MB/s eta 0:00:21\n",
      "   -------------- ------------------------- 80.9/228.5 MB 7.4 MB/s eta 0:00:21\n",
      "   -------------- ------------------------- 81.5/228.5 MB 7.5 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 82.0/228.5 MB 7.7 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 82.8/228.5 MB 8.1 MB/s eta 0:00:19\n",
      "   -------------- ------------------------- 83.7/228.5 MB 8.5 MB/s eta 0:00:18\n",
      "   -------------- ------------------------- 84.4/228.5 MB 8.8 MB/s eta 0:00:17\n",
      "   -------------- ------------------------- 85.6/228.5 MB 9.9 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 86.8/228.5 MB 11.3 MB/s eta 0:00:13\n",
      "   --------------- ------------------------ 88.1/228.5 MB 13.1 MB/s eta 0:00:11\n",
      "   --------------- ------------------------ 89.3/228.5 MB 17.3 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 90.5/228.5 MB 19.3 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 91.7/228.5 MB 22.6 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 92.9/228.5 MB 24.2 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 94.1/228.5 MB 26.2 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 95.3/228.5 MB 27.3 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 96.4/228.5 MB 27.3 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 97.7/228.5 MB 26.2 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 97.8/228.5 MB 24.2 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 99.0/228.5 MB 23.4 MB/s eta 0:00:06\n",
      "   ----------------- --------------------- 100.1/228.5 MB 23.4 MB/s eta 0:00:06\n",
      "   ----------------- --------------------- 101.3/228.5 MB 23.4 MB/s eta 0:00:06\n",
      "   ----------------- --------------------- 102.5/228.5 MB 23.4 MB/s eta 0:00:06\n",
      "   ----------------- --------------------- 103.7/228.5 MB 23.4 MB/s eta 0:00:06\n",
      "   ----------------- --------------------- 104.8/228.5 MB 23.4 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 106.0/228.5 MB 23.4 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 107.2/228.5 MB 23.4 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 108.4/228.5 MB 26.2 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 109.6/228.5 MB 26.2 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 110.4/228.5 MB 26.2 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 111.7/228.5 MB 26.2 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 113.2/228.5 MB 26.2 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 114.5/228.5 MB 26.2 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 114.8/228.5 MB 26.2 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 115.8/228.5 MB 23.4 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 117.0/228.5 MB 23.4 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 118.2/228.5 MB 24.2 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 119.4/228.5 MB 23.4 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 120.6/228.5 MB 23.4 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 121.7/228.5 MB 23.4 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 122.5/228.5 MB 23.4 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 124.1/228.5 MB 24.2 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 125.4/228.5 MB 26.2 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 126.6/228.5 MB 26.2 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 127.8/228.5 MB 26.2 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 128.7/228.5 MB 26.2 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 130.2/228.5 MB 26.2 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 131.7/228.5 MB 27.3 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 133.2/228.5 MB 28.4 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 134.8/228.5 MB 28.4 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 136.1/228.5 MB 31.2 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 137.7/228.5 MB 29.8 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 139.0/228.5 MB 32.7 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 140.5/228.5 MB 32.8 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 142.0/228.5 MB 32.8 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 143.4/228.5 MB 32.8 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 144.8/228.5 MB 31.2 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 146.1/228.5 MB 31.2 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 147.6/228.5 MB 31.2 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 149.0/228.5 MB 31.2 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 150.4/228.5 MB 31.1 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 151.8/228.5 MB 29.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 153.2/228.5 MB 29.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 154.5/228.5 MB 31.1 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 155.8/228.5 MB 29.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 157.2/228.5 MB 29.7 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 158.7/228.5 MB 31.2 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 159.7/228.5 MB 29.7 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 161.4/228.5 MB 29.7 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 162.5/228.5 MB 29.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 164.1/228.5 MB 29.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 165.6/228.5 MB 31.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 167.1/228.5 MB 31.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 168.7/228.5 MB 31.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 169.8/228.5 MB 29.7 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 171.3/228.5 MB 31.2 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 172.6/228.5 MB 31.2 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 174.3/228.5 MB 31.2 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 175.6/228.5 MB 31.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 177.1/228.5 MB 31.1 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 178.4/228.5 MB 29.8 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 179.6/228.5 MB 28.5 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 180.3/228.5 MB 29.7 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 181.6/228.5 MB 28.5 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 182.2/228.5 MB 26.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 183.7/228.5 MB 26.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 185.0/228.5 MB 26.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 186.5/228.5 MB 26.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 187.9/228.5 MB 26.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 189.0/228.5 MB 25.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 190.5/228.5 MB 27.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 191.7/228.5 MB 27.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 193.2/228.5 MB 29.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 194.6/228.5 MB 29.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 196.1/228.5 MB 29.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 197.6/228.5 MB 29.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 199.1/228.5 MB 31.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 200.2/228.5 MB 29.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 201.8/228.5 MB 31.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 202.1/228.5 MB 28.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 203.7/228.5 MB 28.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 204.6/228.5 MB 27.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 206.1/228.5 MB 28.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 207.4/228.5 MB 27.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 208.6/228.5 MB 26.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 209.2/228.5 MB 24.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 209.9/228.5 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 211.2/228.5 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 212.6/228.5 MB 25.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 213.5/228.5 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 215.1/228.5 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 216.6/228.5 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 218.1/228.5 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 219.5/228.5 MB 29.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 221.0/228.5 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 222.0/228.5 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  223.2/228.5 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  224.8/228.5 MB 31.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  225.9/228.5 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  226.9/228.5 MB 28.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.4/228.5 MB 28.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 28.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 28.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 28.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 28.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 228.5/228.5 MB 17.7 MB/s eta 0:00:00\n",
      "Downloading intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl (3.5 MB)\n",
      "   ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 1.2/3.5 MB 25.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 2.5/3.5 MB 31.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.5/3.5 MB 28.1 MB/s eta 0:00:00\n",
      "Downloading tbb-2021.12.0-py3-none-win_amd64.whl (286 kB)\n",
      "   ---------------------------------------- 0.0/286.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 286.4/286.4 kB 17.3 MB/s eta 0:00:00\n",
      "Downloading fsspec-2024.6.0-py3-none-any.whl (176 kB)\n",
      "   ---------------------------------------- 0.0/176.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 176.9/176.9 kB ? eta 0:00:00\n",
      "Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 1.5/1.7 MB 32.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 26.9 MB/s eta 0:00:00\n",
      "Downloading sympy-1.12.1-py3-none-any.whl (5.7 MB)\n",
      "   ---------------------------------------- 0.0/5.7 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 1.6/5.7 MB 33.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 2.7/5.7 MB 28.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.2/5.7 MB 30.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.4/5.7 MB 29.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.7/5.7 MB 26.2 MB/s eta 0:00:00\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: tbb, mpmath, intel-openmp, sympy, networkx, mkl, fsspec, torch\n",
      "Successfully installed fsspec-2024.6.0 intel-openmp-2021.4.0 mkl-2021.4.0 mpmath-1.3.0 networkx-3.3 sympy-1.12.1 tbb-2021.12.0 torch-2.3.0\n",
      "VAE implementation saved to vae.py\n"
     ]
    }
   ],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = self.build_network(input_dim, hidden_dims, latent_dim * 2)\n",
    "        self.decoder = self.build_network(latent_dim, hidden_dims[::-1], input_dim)\n",
    "        \n",
    "    def build_network(self, input_dim, hidden_dims, output_dim):\n",
    "        layers = []\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = h_dim\n",
    "        layers.append(nn.Linear(input_dim, output_dim))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        h = self.encoder(x)\n",
    "        mu, log_var = torch.chunk(h, 2, dim=1)\n",
    "        \n",
    "        # Reparameterization\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        \n",
    "        # Decoding\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        return x_reconstructed, mu, log_var\n",
    "\n",
    "    def loss_function(self, x, x_reconstructed, mu, log_var):\n",
    "        recon_loss = nn.functional.mse_loss(x_reconstructed, x, reduction='sum')\n",
    "        kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        return recon_loss + kl_loss\n",
    "\n",
    "# Сохранение в файл vae.py\n",
    "with open('vae.py', 'w') as f:\n",
    "    f.write(\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = self.build_network(input_dim, hidden_dims, latent_dim * 2)\n",
    "        self.decoder = self.build_network(latent_dim, hidden_dims[::-1], input_dim)\n",
    "        \n",
    "    def build_network(self, input_dim, hidden_dims, output_dim):\n",
    "        layers = []\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = h_dim\n",
    "        layers.append(nn.Linear(input_dim, output_dim))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        h = self.encoder(x)\n",
    "        mu, log_var = torch.chunk(h, 2, dim=1)\n",
    "        \n",
    "        # Reparameterization\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        \n",
    "        # Decoding\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        return x_reconstructed, mu, log_var\n",
    "\n",
    "    def loss_function(self, x, x_reconstructed, mu, log_var):\n",
    "        recon_loss = nn.functional.mse_loss(x_reconstructed, x, reduction='sum')\n",
    "        kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        return recon_loss + kl_loss\n",
    "\"\"\")\n",
    "\n",
    "print(\"VAE implementation saved to vae.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Сохранить рабочую реализацию в файле с расширением .py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранено в ячейке выше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Обучение модели и вывод результатов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 26.5758\n",
      "Epoch 2, Loss: 23.9425\n",
      "Epoch 3, Loss: 23.0145\n",
      "Epoch 4, Loss: 22.4149\n",
      "Epoch 5, Loss: 21.7334\n",
      "Epoch 6, Loss: 21.2266\n",
      "Epoch 7, Loss: 21.1968\n",
      "Epoch 8, Loss: 20.8079\n",
      "Epoch 9, Loss: 20.7365\n",
      "Epoch 10, Loss: 20.6105\n",
      "Epoch 11, Loss: 20.6760\n",
      "Epoch 12, Loss: 20.5671\n",
      "Epoch 13, Loss: 20.5819\n",
      "Epoch 14, Loss: 20.6173\n",
      "Epoch 15, Loss: 20.3113\n",
      "Epoch 16, Loss: 20.2944\n",
      "Epoch 17, Loss: 20.0063\n",
      "Epoch 18, Loss: 19.8835\n",
      "Epoch 19, Loss: 19.5635\n",
      "Epoch 20, Loss: 19.3621\n",
      "Epoch 21, Loss: 19.2727\n",
      "Epoch 22, Loss: 19.2164\n",
      "Epoch 23, Loss: 19.2883\n",
      "Epoch 24, Loss: 19.0760\n",
      "Epoch 25, Loss: 18.9045\n",
      "Epoch 26, Loss: 18.9492\n",
      "Epoch 27, Loss: 18.9664\n",
      "Epoch 28, Loss: 27.4430\n",
      "Epoch 29, Loss: 19.0233\n",
      "Epoch 30, Loss: 18.9498\n",
      "Epoch 31, Loss: 19.0433\n",
      "Epoch 32, Loss: 19.0543\n",
      "Epoch 33, Loss: 18.9208\n",
      "Epoch 34, Loss: 18.7433\n",
      "Epoch 35, Loss: 18.8706\n",
      "Epoch 36, Loss: 18.8364\n",
      "Epoch 37, Loss: 18.7114\n",
      "Epoch 38, Loss: 18.7671\n",
      "Epoch 39, Loss: 18.5604\n",
      "Epoch 40, Loss: 18.7263\n",
      "Epoch 41, Loss: 18.8229\n",
      "Epoch 42, Loss: 18.5512\n",
      "Epoch 43, Loss: 26.5414\n",
      "Epoch 44, Loss: 18.9646\n",
      "Epoch 45, Loss: 18.6893\n",
      "Epoch 46, Loss: 19.0698\n",
      "Epoch 47, Loss: 18.8586\n",
      "Epoch 48, Loss: 19.1609\n",
      "Epoch 49, Loss: 18.9135\n",
      "Epoch 50, Loss: 18.8935\n",
      "Epoch 51, Loss: 18.8896\n",
      "Epoch 52, Loss: 18.8470\n",
      "Epoch 53, Loss: 18.8438\n",
      "Epoch 54, Loss: 18.9451\n",
      "Epoch 55, Loss: 19.1020\n",
      "Epoch 56, Loss: 19.1940\n",
      "Epoch 57, Loss: 19.6851\n",
      "Epoch 58, Loss: 19.3551\n",
      "Epoch 59, Loss: 19.2198\n",
      "Epoch 60, Loss: 20.3233\n",
      "Epoch 61, Loss: 19.5460\n",
      "Epoch 62, Loss: 19.3755\n",
      "Epoch 63, Loss: 19.2364\n",
      "Epoch 64, Loss: 19.2044\n",
      "Epoch 65, Loss: 19.2472\n",
      "Epoch 66, Loss: 19.1015\n",
      "Epoch 67, Loss: 19.0960\n",
      "Epoch 68, Loss: 18.9104\n",
      "Epoch 69, Loss: 18.8840\n",
      "Epoch 70, Loss: 26.7095\n",
      "Epoch 71, Loss: 18.8023\n",
      "Epoch 72, Loss: 18.8471\n",
      "Epoch 73, Loss: 19.8359\n",
      "Epoch 74, Loss: 18.9669\n",
      "Epoch 75, Loss: 19.0358\n",
      "Epoch 76, Loss: 19.1712\n",
      "Epoch 77, Loss: 18.7728\n",
      "Epoch 78, Loss: 18.7009\n",
      "Epoch 79, Loss: 18.8595\n",
      "Epoch 80, Loss: 19.0373\n",
      "Epoch 81, Loss: 18.5979\n",
      "Epoch 82, Loss: 18.6436\n",
      "Epoch 83, Loss: 18.8662\n",
      "Epoch 84, Loss: 18.9389\n",
      "Epoch 85, Loss: 18.9399\n",
      "Epoch 86, Loss: 18.8627\n",
      "Epoch 87, Loss: 18.9821\n",
      "Epoch 88, Loss: 18.5686\n",
      "Epoch 89, Loss: 18.5152\n",
      "Epoch 90, Loss: 18.6719\n",
      "Epoch 91, Loss: 18.4106\n",
      "Epoch 92, Loss: 18.4700\n",
      "Epoch 93, Loss: 18.5646\n",
      "Epoch 94, Loss: 18.6306\n",
      "Epoch 95, Loss: 18.5143\n",
      "Epoch 96, Loss: 18.3408\n",
      "Epoch 97, Loss: 18.7822\n",
      "Epoch 98, Loss: 18.4779\n",
      "Epoch 99, Loss: 18.3914\n",
      "Epoch 100, Loss: 18.3485\n",
      "Test Loss: 18.3374\n"
     ]
    }
   ],
   "source": [
    "# Преобразование данных в тензоры\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# Создание DataLoader для тренировочной и тестовой выборок\n",
    "train_dataset = TensorDataset(X_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Инициализация модели, оптимизатора и функции потерь\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dims = [128, 64, 32]\n",
    "latent_dim = 2\n",
    "vae = VAE(input_dim, hidden_dims, latent_dim)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "# Обучение модели\n",
    "num_epochs = 100\n",
    "vae.train()\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for x_batch in train_loader:\n",
    "        x_batch = x_batch[0]\n",
    "        optimizer.zero_grad()\n",
    "        x_reconstructed, mu, log_var = vae(x_batch)\n",
    "        loss = vae.loss_function(x_batch, x_reconstructed, mu, log_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch+1}, Loss: {train_loss:.4f}')\n",
    "\n",
    "# Вывод результатов\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0\n",
    "    for x_batch in test_loader:\n",
    "        x_batch = x_batch[0]\n",
    "        x_reconstructed, mu, log_var = vae(x_batch)\n",
    "        loss = vae.loss_function(x_batch, x_reconstructed, mu, log_var)\n",
    "        test_loss += loss.item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'Test Loss: {test_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss уменьшался с каждой эпохой, что говорит о том, что модель научилась реконструировать данные и регуляризовать латентное пространство, близкое к априорному распределению. Однако более глубокие выводы нужно будет сделать, если бы в задании требовалось сравнить с другими алгоритмами."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
